Setting up training components...
Loading pretrained weights from /kaggle/input/cub-200-2011/ViT-B_16.npz
✓ Mixed precision training enabled (FP16)
Model initialized with 93.65M parameters
CUB Dataset: 5994 train samples loaded
CUB Dataset: 5794 test samples loaded
Starting training for 100 epochs
Training samples: 5994
Validation samples: 5794
Physical batch size: 4
Gradient accumulation steps: 4
Effective batch size: 16
Epoch 1: 100%|##########| 1498/1498 [33:02<00:00,  1.32s/it, L=8.093, SA=0.01, GL=0.01, PW=0.00, w1=0.0, w2=0.0, w3=0.0]

================================================================================
EPOCH 1 SUMMARY
================================================================================
Learning Rate: 0.000003

Loss Breakdown:
  Total Loss:    8.0930
  SA Loss:       5.3992
  GLCA Loss:     5.4260
  PWCA Loss:     5.3632

Accuracy:
  SA Acc:        0.0082
  GLCA Acc:      0.0050
  PWCA Acc:      0.0033

Uncertainty Weights (raw w_i):
  w1 (SA):       0.0002
  w2 (GLCA):     0.0002
  w3 (PWCA):     0.0002

Effective Loss Weights (1/exp(w_i)):
  Weight SA:     0.9998
  Weight GLCA:   0.9998
  Weight PWCA:   0.9998

⚠️  WARNING: SA accuracy is very low (0.82%)!
================================================================================
Epoch 2: 100%|##########| 1498/1498 [33:01<00:00,  1.32s/it, L=8.037, SA=0.01, GL=0.01, PW=0.00, w1=0.0, w2=0.0, w3=0.0]


================================================================================
EPOCH 2 SUMMARY
================================================================================
Learning Rate: 0.000006

Loss Breakdown:
  Total Loss:    8.0369
  SA Loss:       5.3550
  GLCA Loss:     5.3795
  PWCA Loss:     5.3571

Accuracy:
  SA Acc:        0.0085
  GLCA Acc:      0.0072
  PWCA Acc:      0.0035

Uncertainty Weights (raw w_i):
  w1 (SA):       0.0014
  w2 (GLCA):     0.0014
  w3 (PWCA):     0.0014

Effective Loss Weights (1/exp(w_i)):
  Weight SA:     0.9986
  Weight GLCA:   0.9986
  Weight PWCA:   0.9986

⚠️  WARNING: SA accuracy is very low (0.85%)!
================================================================================
Epoch 3: 100%|##########| 1498/1498 [33:01<00:00,  1.32s/it, L=7.919, SA=0.01, GL=0.01, PW=0.01, w1=0.0, w2=0.0, w3=0.0]


================================================================================
EPOCH 3 SUMMARY
================================================================================
Learning Rate: 0.000009

Loss Breakdown:
  Total Loss:    7.9189
  SA Loss:       5.2606
  GLCA Loss:     5.2907
  PWCA Loss:     5.3340

Accuracy:
  SA Acc:        0.0125
  GLCA Acc:      0.0127
  PWCA Acc:      0.0053

Uncertainty Weights (raw w_i):
  w1 (SA):       0.0037
  w2 (GLCA):     0.0037
  w3 (PWCA):     0.0037

Effective Loss Weights (1/exp(w_i)):
  Weight SA:     0.9963
  Weight GLCA:   0.9963
  Weight PWCA:   0.9963

⚠️  WARNING: SA accuracy is very low (1.25%)!
================================================================================
Epoch 4: 100%|##########| 1498/1498 [33:01<00:00,  1.32s/it, L=7.758, SA=0.03, GL=0.02, PW=0.01, w1=0.0, w2=0.0, w3=0.0]


================================================================================
EPOCH 4 SUMMARY
================================================================================
Learning Rate: 0.000013

Loss Breakdown:
  Total Loss:    7.7579
  SA Loss:       5.1402
  GLCA Loss:     5.1648
  PWCA Loss:     5.3013

Accuracy:
  SA Acc:        0.0255
  GLCA Acc:      0.0202
  PWCA Acc:      0.0078

Uncertainty Weights (raw w_i):
  w1 (SA):       0.0072
  w2 (GLCA):     0.0072
  w3 (PWCA):     0.0073

Effective Loss Weights (1/exp(w_i)):
  Weight SA:     0.9928
  Weight GLCA:   0.9928
  Weight PWCA:   0.9927

⚠️  WARNING: SA accuracy is very low (2.55%)!
================================================================================
Epoch 5: 100%|##########| 1498/1498 [33:01<00:00,  1.32s/it, L=7.595, SA=0.05, GL=0.04, PW=0.01, w1=0.0, w2=0.0, w3=0.0]


================================================================================
EPOCH 5 SUMMARY
================================================================================
Learning Rate: 0.000016

Loss Breakdown:
  Total Loss:    7.5947
  SA Loss:       5.0134
  GLCA Loss:     5.0498
  PWCA Loss:     5.2717

Accuracy:
  SA Acc:        0.0522
  GLCA Acc:      0.0444
  PWCA Acc:      0.0110

Uncertainty Weights (raw w_i):
  w1 (SA):       0.0118
  w2 (GLCA):     0.0118
  w3 (PWCA):     0.0121

Effective Loss Weights (1/exp(w_i)):
  Weight SA:     0.9883
  Weight GLCA:   0.9883
  Weight PWCA:   0.9880

⚠️  WARNING: SA accuracy is very low (5.22%)!
================================================================================
Evaluating val:   0%|                                                      | 0/1449 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/kaggle/working/clau_dual_cross_attention_learning/train.py", line 1088, in <module>
    main()
  File "/kaggle/working/clau_dual_cross_attention_learning/train.py", line 1080, in main
    trainer.train()
  File "/kaggle/working/clau_dual_cross_attention_learning/train.py", line 862, in train
    val_metrics = self.evaluate(model, val_loader, criterion, "val")
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/clau_dual_cross_attention_learning/train.py", line 620, in evaluate
    return self._evaluate_fgvc(model, data_loader, criterion, split)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/clau_dual_cross_attention_learning/train.py", line 639, in _evaluate_fgvc
    outputs = model(images, paired_images=None)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: DualCrossAttentionViT.forward() got an unexpected keyword argument 'paired_images'